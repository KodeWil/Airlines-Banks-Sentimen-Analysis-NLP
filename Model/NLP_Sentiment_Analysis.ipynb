{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Sentiment-Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaQZrNvjwIUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import Project Data\n",
        "# !git clone https://github.com/KodeWil/Airlines-Banks-Sentimen-Analysis-NLP.git"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB8y_UysxNV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1d6b766-00b6-4559-86b1-65259aa65e09"
      },
      "source": [
        "import pandas as pd \n",
        "%cd /content/Airlines-Banks-Sentimen-Analysis-NLP/Data\n",
        "tweetsFrame = pd.read_csv(\"Tweets.csv\")\n",
        "\n",
        "#Extract the text and the sentiment associated to it\n",
        "infoFrame = tweetsFrame[[\"text\", \"airline_sentiment\"]]\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Airlines-Banks-Sentimen-Analysis-NLP/Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBOugWFczGXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replace the airlines name to a standar name \"enterprise\"\n",
        "infoFrame[\"airline_sentiment\"] = infoFrame[\"airline_sentiment\"].str.replace(\"neutral\", \"1\", case = False)\n",
        "infoFrame[\"airline_sentiment\"] = infoFrame[\"airline_sentiment\"].str.replace(\"positive\", \"2\", case = False)\n",
        "infoFrame[\"airline_sentiment\"] = infoFrame[\"airline_sentiment\"].str.replace(\"negative\", \"0\", case = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVlNx459-Z3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "companyNames = [\"@VirginAmerica\", \"@united\", \"@SouthwestAir\", \"@JetBlue\", \"@UsAirways\", \"@AmericanAir\"]\n",
        "#Replace company names for a generic name \"company\"\n",
        "for company in companyNames:\n",
        "  infoFrame[\"text\"] = infoFrame[\"text\"].str.replace(company, \"company\", case = False)\n",
        "\n",
        "#Standarized data\n",
        "infoFrame['airline_sentiment'] = infoFrame['airline_sentiment'].replace({'neutral':'1', 'positive':'2', 'negative': '0'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoID1L2z5Hfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f60732db-a71e-4b51-e68e-2f1e2e60a2b4"
      },
      "source": [
        "#Divide the data in train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = infoFrame[\"text\"]\n",
        "y = infoFrame[\"airline_sentiment\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=None)\n",
        "#Encode sentiment type and tokenize sentences\n",
        "import tensorflow as tf\n",
        "from keras.layers import Flatten, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "#Tokenize sentences\n",
        "oov_tok = \"<OOV>\" #Define token for missing words\n",
        "embedding_dim = 50\n",
        "max_length = 20\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "vocab_size = 20000 #Define the length of words to save\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) #Set the tokenizer\n",
        "tokenizer.fit_on_texts(X_train) #Fit it with train set\n",
        "word_index = tokenizer.word_index #Save the vocabulary\n",
        "#Tokenize text to sequences (train)\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "#Tokenize text to sequences (test)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_48ckzqlC3el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgbxSrbYDW0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "X_train = np.asarray(train_padded, dtype='i4')\n",
        "y_train = np.asarray(y_train, dtype='i4')\n",
        "X_test = np.asarray(test_padded, dtype='i4')\n",
        "y_test = np.asarray(y_test, dtype='i4')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p9yBp0coCxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa01800a-0c5e-4b90-d9b8-5702efcb1166"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6338797814207651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArLtjuACDuMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "cf2e5519-f4ff-42a8-de89-f560c5720e6e"
      },
      "source": [
        "#train model\n",
        "num_epochs = 15\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "366/366 - 4s - loss: -5.0135e+00 - accuracy: 0.6062 - val_loss: -3.9631e+01 - val_accuracy: 0.6182\n",
            "Epoch 2/15\n",
            "366/366 - 4s - loss: -1.3284e+03 - accuracy: 0.6498 - val_loss: -3.8817e+03 - val_accuracy: 0.6130\n",
            "Epoch 3/15\n",
            "366/366 - 4s - loss: -1.9682e+04 - accuracy: 0.6442 - val_loss: -3.6984e+04 - val_accuracy: 0.6301\n",
            "Epoch 4/15\n",
            "366/366 - 4s - loss: -1.1186e+05 - accuracy: 0.6232 - val_loss: -1.6021e+05 - val_accuracy: 0.6209\n",
            "Epoch 5/15\n",
            "366/366 - 4s - loss: -3.8484e+05 - accuracy: 0.6351 - val_loss: -4.7532e+05 - val_accuracy: 0.6342\n",
            "Epoch 6/15\n",
            "366/366 - 4s - loss: -9.9765e+05 - accuracy: 0.6417 - val_loss: -1.0803e+06 - val_accuracy: 0.6216\n",
            "Epoch 7/15\n",
            "366/366 - 4s - loss: -2.1344e+06 - accuracy: 0.6180 - val_loss: -2.1394e+06 - val_accuracy: 0.6175\n",
            "Epoch 8/15\n",
            "366/366 - 4s - loss: -3.9999e+06 - accuracy: 0.6304 - val_loss: -3.7985e+06 - val_accuracy: 0.6189\n",
            "Epoch 9/15\n",
            "366/366 - 4s - loss: -6.8246e+06 - accuracy: 0.6319 - val_loss: -6.2515e+06 - val_accuracy: 0.6202\n",
            "Epoch 10/15\n",
            "366/366 - 4s - loss: -1.0883e+07 - accuracy: 0.6528 - val_loss: -9.6198e+06 - val_accuracy: 0.6175\n",
            "Epoch 11/15\n",
            "366/366 - 4s - loss: -1.6460e+07 - accuracy: 0.6436 - val_loss: -1.4193e+07 - val_accuracy: 0.6151\n",
            "Epoch 12/15\n",
            "366/366 - 4s - loss: -2.3953e+07 - accuracy: 0.6491 - val_loss: -2.0313e+07 - val_accuracy: 0.6178\n",
            "Epoch 13/15\n",
            "366/366 - 4s - loss: -3.3748e+07 - accuracy: 0.6531 - val_loss: -2.8100e+07 - val_accuracy: 0.6168\n",
            "Epoch 14/15\n",
            "366/366 - 4s - loss: -4.6235e+07 - accuracy: 0.6329 - val_loss: -3.7860e+07 - val_accuracy: 0.6168\n",
            "Epoch 15/15\n",
            "366/366 - 4s - loss: -6.1614e+07 - accuracy: 0.6544 - val_loss: -4.9717e+07 - val_accuracy: 0.2459\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}